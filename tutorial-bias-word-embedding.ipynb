{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Responsible Data Science\n",
    "# Bias in Word Embeddings\n",
    "\n",
    "https://ethically.ai/word-embedding\n",
    "\n",
    "### Powerd by [`ethically`](https://docs.ethically.ai/) - Toolkit for Auditing and Mitigating Bias and Fairness of Machine Learning Systems üîéü§ñüß∞\n",
    "\n",
    "## by Shlomi Hod (shlomi[dot]hod[at]uni-potsdam[dot]de)\n",
    "\n",
    "\n",
    "![](images/banner.png)\n",
    "\n",
    "### Legend:\n",
    "# üíé Important\n",
    "# ‚ö° Be Aware - Debated issue / interpret carefully / simplicity over precision\n",
    "# üõ†Ô∏è Setup/Technical (a.k.a \"the code is not important\")\n",
    "# üíª Hands-On - Your turn!\n",
    "# ü¶Ñ Out of Scope\n",
    "\n",
    "![](images/banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Install `ethically`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user ethically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Validate Installation of `ethically`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ethically\n",
    "\n",
    "ethically.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation: Why to Learn Word Embeddings?\n",
    "\n",
    "### [NLP (Natural Language Processing)](https://en.wikipedia.org/wiki/Natural_language_processing) - **Very partial** list of tasks\n",
    "\n",
    "\n",
    "### 1. Classification\n",
    "- Sentiment Analysis\n",
    "- Hiring decision making by CV\n",
    "\n",
    "### 2. Information retrieval\n",
    "Search engine\n",
    "\n",
    "### 3. Coreference Resolution\n",
    "![](images/corefexample.png)\n",
    "<small>Source: [Stanford Natural Language Processing Group](https://nlp.stanford.edu/projects/coref.shtml)\n",
    "\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "\n",
    "<center>\n",
    "<h1> Esessional Question - How to encode Words into Numbers?</h1>\n",
    "</center>\n",
    "\n",
    "### Idea: Bag of Words (for a document)\n",
    "![](images/bow.png)\n",
    "<small>Source: Zheng, A.& Casari, A. (2018). Feature Engineering for Machine Learning. O'Reilly Media.</small>\n",
    "\n",
    "### One-Hot Representation - The Issue with Text\n",
    "\n",
    "![](https://www.tensorflow.org/images/audio-image-text.png)\n",
    "<small>Source: [Tensorflow Documentation](https://www.tensorflow.org/tutorials/representation/word2vec)</small>\n",
    "\n",
    "\n",
    "## üíé Idea: Embedding a word in a n-dimensional space\n",
    "\n",
    "### Distributional Hypothesis\n",
    "> \"a word is characterized by the company it keeps\" - [John Rupert Firth](https://en.wikipedia.org/wiki/John_Rupert_Firth)\n",
    "\n",
    "#### ü¶Ñ Training: using *word-context* relationships from a corpus. See: [The Illustrated Word2vec by Jay Alammar](http://jalammar.github.io/illustrated-word2vec/)\n",
    "\n",
    "### Distance ~ Meaning Similarity\n",
    "\n",
    "## ü¶Ñ Examples (algorithms and pre-trained models)\n",
    "- [Word2Vec](https://code.google.com/archive/p/word2vec/)\n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "- [fastText](https://fasttext.cc/)\n",
    "- [ELMo](https://allennlp.org/elmo) (contextualized)\n",
    "\n",
    "### ü¶Ñ State of the Art\n",
    "[The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)\n",
    "](http://jalammar.github.io/illustrated-bert/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's play with Word2Vec word embedding...!\n",
    "\n",
    "[Word2Vec](https://code.google.com/archive/p/word2vec/) - Google News - 100B tokens, 3M vocab, cased, 300d vectors - only lowercase vocab extracted\n",
    "\n",
    "Loaded using [ethically](http://docs.ethically.ai) package, the function [`ethically.we.load_w2v_small`]() returns a [gensim](https://radimrehurek.com/gensim/)'s [KeyedVectors](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è‚ö° ignore warnings\n",
    "# generally, you shouldn't do that, but for this tutorial we'll do so for the sake of simplicity\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethically.we import load_w2v_small\n",
    "\n",
    "w2v_small = load_w2v_small()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary size\n",
    "\n",
    "len(w2v_small.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vector of the word \"home\"\n",
    "\n",
    "print('home =', w2v_small['home'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the word embedding dimension, in this case, is 300\n",
    "\n",
    "len(w2v_small['home'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the words are normalized (=have norm equal to one as vectors)\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "norm(w2v_small['home'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è make sure that all the vectors are normalized!\n",
    "\n",
    "from numpy.testing import assert_almost_equal\n",
    "\n",
    "length_vectors = norm(w2v_small.vectors, axis=1)\n",
    "\n",
    "assert_almost_equal(actual=length_vectors,\n",
    "                    desired=1,\n",
    "                    decimal=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíé Demo - Mesuring Distance between Words\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7e/Sphere_wireframe_10deg_6r.svg/480px-Sphere_wireframe_10deg_6r.svg.png)\n",
    "\n",
    "### Mesure of Similiarty: [Cosine Similariy](https://en.wikipedia.org/wiki/Cosine_similarity) = \n",
    "#### Measures the cosine of the angle between two vecotrs.\n",
    "#### Ranges between 1 (same vector) to -1 (opposite/antipode vector)\n",
    "\n",
    "#### In Python, for normalized vectors (Numpy's array), use the `@`(at) operator!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small['cat'] @ w2v_small['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small['cat'] @ w2v_small['cats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import acos, degrees\n",
    "\n",
    "degrees(acos(w2v_small['cat'] @ w2v_small['cats']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small['cat'] @ w2v_small['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees(acos(w2v_small['cat'] @ w2v_small['dog']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small['cat'] @ w2v_small['cow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees(acos(w2v_small['cat'] @ w2v_small['cow']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small['cat'] @ w2v_small['graduated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees(acos(w2v_small['cat'] @ w2v_small['graduated']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Demo - Visualization Word Embedding in 2D using T-SNE \n",
    "\n",
    "<small>Source: [Google's Seedbank](https://research.google.com/seedbank/seed/pretrained_word_embeddings)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "# take the most common words in the corpus between 200 and 600\n",
    "words = [word for word in w2v_small.index2word[200:600]]\n",
    "\n",
    "# convert the words to vectors\n",
    "embeddings = [w2v_small[word] for word in words]\n",
    "\n",
    "# perform T-SNE\n",
    "words_embedded = TSNE(n_components=2).fit_transform(embeddings)\n",
    "\n",
    "# ... and visualize!\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, label in enumerate(words):\n",
    "    x, y = words_embedded[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',\n",
    "                 ha='right', va='bottom', size=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo - [Tensorflow Embedding Projector](http://projector.tensorflow.org)\n",
    "\n",
    "Be cautious: It is easy to see \"patterns\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo - Most Similar\n",
    "\n",
    "What are the most simlar words (=closer) to a given word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small.most_similar('cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo - Doesn't Match\n",
    "\n",
    "Given a list of words, which one doesn't match?\n",
    "\n",
    "The word further away from the mean of all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small.doesnt_match('breakfast cereal dinner lunch'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo - Vector Arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nature + science = ?\n",
    "\n",
    "w2v_small.most_similar(positive=['nature', 'science'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíé More Vector Arithmetic\n",
    "\n",
    "![](https://www.tensorflow.org/images/linear-relationships.png)\n",
    "<small>Source: [Tensorflow Documentation](https://www.tensorflow.org/tutorials/representation/word2vec)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo - Vector Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# man:king :: woman:?\n",
    "# king - man + woman = ?\n",
    "\n",
    "w2v_small.most_similar(positive=['king', 'woman'],\n",
    "                       negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small.most_similar(positive=['east', 'west'],\n",
    "                       negative=['south'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small.most_similar(positive=['big', 'smaller'],\n",
    "                       negative=['small'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small.most_similar(positive=['big', 'smaller'],\n",
    "                       negative=['small'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Think about a DIRECTION in word embedding as a RELATION\n",
    "\n",
    "# $\\overrightarrow{she} - \\overrightarrow{he}$\n",
    "# $\\overrightarrow{smaller} - \\overrightarrow{small}$\n",
    "# $\\overrightarrow{Spain} - \\overrightarrow{Madrid}$\n",
    "\n",
    "\n",
    "### ‚ö° Direction is not a word vector by itself!\n",
    "\n",
    "### ‚ö°ü¶Ñ Keep in mind the word embedding was generated by learning the co-occurrence of words, so the fact that it *empirically* exhibit \"concept arithmetic\", it doesn't necessarily mean it learned it! In fact, it seems it didn't.\n",
    "See: [king - man + woman is queen; but why? by Piotr Migda≈Ç](https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html)\n",
    "\n",
    "### ü¶Ñ [Demo - Word Analogies Visualizer by Julia Bazi≈Ñska](https://lamyiowce.github.io/word2viz/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo - Shift Context\n",
    "<small>Source: [Google's Seedbank](https://research.google.com/seedbank/seed/pretrained_word_embeddings)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_context(sentence, from_context, to_context):\n",
    "    new_sentence = []\n",
    "    for word in sentence.split():\n",
    "        if word in w2v_small:\n",
    "            word = w2v_small.most_similar(positive=[word, to_context],\n",
    "                                          negative=[from_context])[0][0]\n",
    "        new_sentence.append(word)\n",
    "\n",
    "    return ' '.join(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'restaurant serving coffee with cream and bread'\n",
    "\n",
    "print(shift_context(sentence, 'regular', 'fancy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/banner.png)\n",
    "\n",
    "# Gender Bias\n",
    "Keep in mind, the data is from Google News, the writers are professional journalists.\n",
    "\n",
    "### Bolukbasi Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam T. Kalai. [Man is to computer programmer as woman is to homemaker? debiasing word embeddings](https://arxiv.org/abs/1607.06520). NIPS 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender appropriate he-she analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# she:sister :: he:?\n",
    "# sister - she + he = ?\n",
    "\n",
    "w2v_small.most_similar(positive=['sister', 'he'],\n",
    "                       negative=['she'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "queen-king\n",
    "waitress-waiter\n",
    "sister-brother\n",
    "mother-father\n",
    "ovarian_cancer-prostate_cancer\n",
    "convent-monastery\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender stereotype he-she analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small.most_similar(positive=['nurse', 'he'],\n",
    "                       negative=['she'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sewing-carpentry\n",
    "nurse-doctor\n",
    "blond-burly\n",
    "giggle-chuckle\n",
    "sassy-snappy\n",
    "volleyball-football\n",
    "register_nurse-physician\n",
    "interior_designer-architect\n",
    "feminism-conservatism\n",
    "vocalist-guitarist\n",
    "diva-superstar\n",
    "cupcakes-pizzas\n",
    "housewife-shopkeeper\n",
    "softball-baseball\n",
    "cosmetics-pharmaceuticals\n",
    "petite-lanky\n",
    "charming-affable\n",
    "hairdresser-barber\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Be Aware: According to a recent paper, it seems that the method of generating analogies enforce producing gender sterotype ones!\n",
    "\n",
    "Nissim, M., van Noord, R., van der Goot, R. (2019). [Fair is Better than Sensational: Man is to Doctor as Woman is to Doctor](https://arxiv.org/abs/1905.09866).\n",
    "\n",
    "... and a [Twitter thread](https://twitter.com/adamfungi/status/1133865428663635968) between the authors of the two papares.\n",
    "\n",
    "## My takeaway (and not only): Analogies are not approriate method to observe bias in word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sewing-carpentry\n",
    "nurse-surgeon\n",
    "blond-burly\n",
    "giggle-chuckle\n",
    "sassy-snappy\n",
    "volleyball-football\n",
    "register_nurse-physician\n",
    "interior_designer-architect\n",
    "feminism-conservatism\n",
    "vocalist-guitarist\n",
    "diva-superstar\n",
    "cupcakes-pizzas\n",
    "housewife-shopkeeper\n",
    "softball-baseball\n",
    "cosmetics-pharmaceuticals\n",
    "petite-lanky\n",
    "charming-affable\n",
    "hairdresser-barber\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíé What can we take from analogies? Gender Direction\n",
    "\n",
    "# $\\overrightarrow{she} - \\overrightarrow{he}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_direction = w2v_small['she'] - w2v_small['he']\n",
    "\n",
    "gender_direction /= norm(gender_direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_direction @ w2v_small['architect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_direction @ w2v_small['interior_designer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö°Interprete carefully: The word *architect* appears in more contexts with *he* than with *she*, and vice versa for *interior designer*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü¶Ñ In practice, we calculate the gender direction using multiple definitional pair of words for better estimation (words may have more than one meaning):\n",
    "\n",
    "- woman - man\n",
    "- girl - boy\n",
    "- she - he\n",
    "- mother - father\n",
    "- daughter - son\n",
    "- gal - guy\n",
    "- female - male\n",
    "- her - his\n",
    "- herself - himself\n",
    "- Mary - John"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Try some words by yourself\n",
    "‚ö° Keep in mind: You are performing exploratory data analysis, and not evaluate systematically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_direction @ w2v_small['word']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíé So What?\n",
    "\n",
    "### Downstream Application\n",
    "\n",
    "### Toy Example - Search Engine Ranking\n",
    "\n",
    "- \"MIT PhD. Student\"\n",
    "- \"doctoral candidate\" ~ \"PhD. student\"\n",
    "- John:computer programmer :: Mary:homemaker\n",
    "\n",
    "### Universal Embeddings\n",
    "- Pre-trained on a large corpus\n",
    "- Plugged in downstream task models (sentimental analysis, classification, translation ‚Ä¶)\n",
    "- Improvement of performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Bias in Word Embedding\n",
    "\n",
    "# Think-Pair-Shar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "# Basic Ideas: Use neutral-gender words!\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "# Professions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethically.we import GenderBiasWE\n",
    "\n",
    "w2v_small_gender_bias = GenderBiasWE(w2v_small,\n",
    "                                     only_lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_bias.positive_end, w2v_small_gender_bias.negative_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender direction\n",
    "w2v_small_gender_bias.direction[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethically.we.data import BOLUKBASI_DATA\n",
    "\n",
    "neutral_profession_names = BOLUKBASI_DATA['gender']['neutral_profession_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_profession_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(neutral_profession_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same of using the @ operator on the bias direction\n",
    "\n",
    "w2v_small_gender_bias.project_on_direction(neutral_profession_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "f, ax = plt.subplots(1, figsize=(10, 10))\n",
    "\n",
    "w2v_small_gender_bias.plot_projection_scores(n_extreme=20, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the projection of occupation words on the gender direction related to the real world?\n",
    "\n",
    "Let's take the percentage of female in various occupations from the Labor Force Statistics of 2017 Population Survey.\n",
    "\n",
    "Taken from: https://arxiv.org/abs/1804.06876"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethically.we.data import OCCUPATION_FEMALE_PRECENTAGE\n",
    "\n",
    "OCCUPATION_FEMALE_PRECENTAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(10, 8))\n",
    "\n",
    "w2v_small_gender_bias.plot_factual_association(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Bias Measure\n",
    "\n",
    "1. Project each **neutral profession names** on the gender direction\n",
    "2. Calculate the absolute value of each projection\n",
    "3. Average it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using ethically\n",
    "\n",
    "w2v_small_gender_bias.calc_direct_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what ethically does:\n",
    "\n",
    "neutral_profession_projections = [w2v_small[word] @ w2v_small_gender_bias.direction\n",
    "                                  for word in neutral_profession_names]\n",
    "\n",
    "abs_neutral_profession_projections = [abs(proj) for proj in neutral_profession_projections]\n",
    "\n",
    "sum(abs_neutral_profession_projections) / len(abs_neutral_profession_projections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indirect Bias Measure - EXTRA\n",
    "Similarity due to shared \"gender direction\" projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_bias.generate_closest_words_indirect_bias('softball',\n",
    "                                                           'football')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debias\n",
    "\n",
    "### Neutralize\n",
    "\n",
    "In this case, we will remove the gender projection from all the words, except the neutral-gender ones, and then normalize.\n",
    "\n",
    "ü¶Ñ We need to \"learn\" what are the gender-specific words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias = w2v_small_gender_bias.debias(method='neutralize', inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('home:',\n",
    "      'before =', w2v_small_gender_bias.model['home'] @ w2v_small_gender_bias.direction,\n",
    "      'after = ', w2v_small_gender_debias.model['home'] @ w2v_small_gender_debias.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('man:',\n",
    "      'before =', w2v_small_gender_bias.model['man'] @ w2v_small_gender_bias.direction,\n",
    "      'after = ', w2v_small_gender_debias.model['man'] @ w2v_small_gender_debias.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('woman:',\n",
    "      'before =', w2v_small_gender_bias.model['woman'] @ w2v_small_gender_bias.direction,\n",
    "      'after = ', w2v_small_gender_debias.model['woman'] @ w2v_small_gender_debias.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.calc_direct_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(10, 10))\n",
    "\n",
    "w2v_small_gender_debias.plot_projection_scores(n_extreme=20, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(10, 8))\n",
    "\n",
    "w2v_small_gender_debias.plot_factual_association(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equalize EXTRA\n",
    "\n",
    "- Do you see that `man` and `woman` have a different projection on the gender direction? \n",
    "\n",
    "- It might cause to different similarity (distance) to neutral words, such as to `kitchen`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.model['man'] @ w2v_small_gender_debias.model['kitchen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.model['woman'] @ w2v_small_gender_debias.model['kitchen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLUKBASI_DATA['gender']['equalize_pairs'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Debias = Neutralize + Equalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias = w2v_small_gender_bias.debias(method='hard', inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('home:',\n",
    "      'before =', w2v_small_gender_bias.model['home'] @ w2v_small_gender_bias.direction,\n",
    "      'after = ', w2v_small_gender_debias.model['home'] @ w2v_small_gender_debias.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('man:',\n",
    "      'before =', w2v_small_gender_bias.model['man'] @ w2v_small_gender_bias.direction,\n",
    "      'after = ', w2v_small_gender_debias.model['man'] @ w2v_small_gender_debias.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('woman:',\n",
    "      'before =', w2v_small_gender_bias.model['woman'] @ w2v_small_gender_bias.direction,\n",
    "      'after = ', w2v_small_gender_debias.model['woman'] @ w2v_small_gender_debias.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.calc_direct_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.model['man'] @ w2v_small_gender_debias.model['kitchen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.model['woman'] @ w2v_small_gender_debias.model['kitchen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(10, 10))\n",
    "\n",
    "w2v_small_gender_debias.plot_projection_scores(n_extreme=20, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Preformances\n",
    "\n",
    "After debiasing, the performance of the word embedding, using standard benchmarks, get only slightly worse!\n",
    "\n",
    "### ‚ö†Ô∏è It might take few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_bias.evaluate_word_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.evaluate_word_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/banner.png)\n",
    "\n",
    "# üíé So What?\n",
    "\n",
    "We removed the gender bias, **as we defined it**, in a word embedding - Is there any impact on a downstream application?\n",
    "\n",
    "\n",
    "### Zhao, J., Wang, T., Yatskar, M., Ordonez, V., & Chang, K. W. (2018). [Gender bias in coreference resolution: Evaluation and debiasing methods](https://par.nsf.gov/servlets/purl/10084252). NAACL-HLT 2018.\n",
    "\n",
    "\n",
    "#### WinoBias Dataset\n",
    "![](images/coref-example.png)\n",
    "\n",
    "\n",
    "#### Stereotypical Occupations (the source of `ethically.we.data.OCCUPATION_FEMALE_PRECENTAGE`)\n",
    "![](images/coref-occupations.png)\n",
    "\n",
    "#### Results\n",
    "![](images/coref-results.png)\n",
    "\n",
    "\n",
    "EE = UW End-to-end Neural Coreference Resolution System\n",
    "\n",
    "\n",
    "### Zhao, J., Zhou, Y., Li, Z., Wang, W., & Chang, K. W. (2018). [Learning gender-neutral word embeddings](https://arxiv.org/pdf/1809.01496.pdf). EMNLP 2018.\n",
    "\n",
    "#### Another debias method (tailor-made for GloVe training process)\n",
    "\n",
    "![](images/gn-glove-results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/banner.png)\n",
    "\n",
    "# üíéüíé Meta \"So What?\" - I\n",
    "\n",
    "## How should we definition of \"bias\" in word embedding?\n",
    "\n",
    "### 1. Intrinsic (e.g., direct bias)\n",
    "\n",
    "### 2. External - Downstream application (e.g., coreference resolution, classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/banner.png)\n",
    "\n",
    "# üíé Have we really removed the bias?\n",
    "\n",
    "Let's look on another metric, called **WEAT** (Word Embedding Association Test) which is inspired by **IAT** (Implicit-Association Test) from Pyschology.\n",
    "\n",
    "### Caliskan, A., Bryson, J. J., & Narayanan, A. (2017). [Semantics derived automatically from language corpora contain human-like biases.](http://www.cs.bath.ac.uk/~jjb/ftp/CaliskanEtAl-authors-full.pdf) Science, 356(6334), 183-186.\n",
    "\n",
    "\n",
    "### Ingredients\n",
    "\n",
    "1. Target words (e.g., Male ve. Female)\n",
    "\n",
    "2. Attribute words (e.g., Math vs. Arts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from ethically.we.weat import WEAT_DATA\n",
    "\n",
    "\n",
    "# B. A. Nosek, M. R. Banaji, A. G. Greenwald, Math=male, me=female, therefore math‚â†me.,\n",
    "# Journal of Personality and Social Psychology 83, 44 (2002).\n",
    "weat_gender_science_arts = deepcopy(WEAT_DATA[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è filter words from the original IAT experiment that are not presend in the reduced Word2Vec model\n",
    "\n",
    "from ethically.we.weat import _filter_by_model_weat_stimuli\n",
    "\n",
    "_filter_by_model_weat_stimuli(weat_gender_science_arts, w2v_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weat_gender_science_arts['first_attribute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weat_gender_science_arts['second_attribute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weat_gender_science_arts['first_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weat_gender_science_arts['second_target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recipe\n",
    "\n",
    "‚ûï Male x Science\n",
    "\n",
    "‚ûñ Male x Arts\n",
    "\n",
    "‚ûñ Female x Science\n",
    "\n",
    "‚ûï Female x Arts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_combination_similiarity(model, attribute, target):\n",
    "    score = 0\n",
    "\n",
    "    for attribute_word in attribute['words']:\n",
    "\n",
    "        for target_word in target['words']:\n",
    "\n",
    "            score += w2v_small.similarity(attribute_word,\n",
    "                                          target_word)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_science_score = calc_combination_similiarity(w2v_small,\n",
    "                                                  weat_gender_science_arts['first_attribute'],\n",
    "                                                  weat_gender_science_arts['first_target'])\n",
    "\n",
    "male_science_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_arts_score = calc_combination_similiarity(w2v_small,\n",
    "                                               weat_gender_science_arts['first_attribute'],\n",
    "                                               weat_gender_science_arts['second_target'])\n",
    "\n",
    "male_arts_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_science_score = calc_combination_similiarity(w2v_small,\n",
    "                                                    weat_gender_science_arts['second_attribute'],\n",
    "                                                    weat_gender_science_arts['first_target'])\n",
    "\n",
    "female_science_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_arts_score = calc_combination_similiarity(w2v_small,\n",
    "                                                 weat_gender_science_arts['second_attribute'],\n",
    "                                                 weat_gender_science_arts['second_target'])\n",
    "\n",
    "female_arts_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_science_score - male_arts_score - female_science_score + female_arts_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(weat_gender_science_arts['first_attribute']['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(male_science_score - male_arts_score - female_science_score + female_arts_score) / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethically.we import calc_all_weat\n",
    "\n",
    "calc_all_weat(w2v_small, [weat_gender_science_arts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° Important Note: Our results are a bit different because we use a reduced Word2Vec.\n",
    "\n",
    "\n",
    "### Results from the Paper (computed on the complete Word2Vec):\n",
    "\n",
    "![](images/weat-w2v.png)\n",
    "\n",
    "\n",
    "### ‚ö°Caveats regarding comparing WEAT to the IAT\n",
    "\n",
    "- Individuals (IAT) vs. Words (WEAT)\n",
    "- Therefore, the meanings of the effect size and p-value are totally different!\n",
    "\n",
    "### ‚ö°ü¶Ñ The definition of the WEAT score is structured differently (but it is computationally equivalent). The original formulation matters to compute the p-value. Refer to the paper for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's go back to our question - did we removed the bias?\n",
    "\n",
    "### Gonen, H., & Goldberg, Y. (2019). [Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them](https://arxiv.org/pdf/1903.03862.pdf). arXiv preprint arXiv:1903.03862.\n",
    "\n",
    "They used multiple methods, we'll show only two:\n",
    "1. WEAT\n",
    "2. Neutral words clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_bias.calc_direct_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.calc_direct_bias()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. WEAT - before and after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/weat-experiment.png)\n",
    "\n",
    "See `ethically` [demo page on word embedding](https://docs.ethically.ail/notebooks/demo-word-embedding-bias.html#first-experiment-weat-before-and-after-debias) for a complete example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Clustering Neutral Gender Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vocab = set(w2v_small_gender_bias.model.vocab.keys())\n",
    "\n",
    "# ü¶Ñ how we got these words - read the Bolukbasi's paper for details\n",
    "all_gender_specific_words = set(BOLUKBASI_DATA['gender']['specific_full_with_definitional_equalize'])\n",
    "\n",
    "all_gender_neutral_words = w2v_vocab - all_gender_specific_words\n",
    "\n",
    "print('#vocab =', len(w2v_vocab),\n",
    "      '#specific =', len(all_gender_specific_words),\n",
    "      '#neutral =', len(all_gender_neutral_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_words_gender_projections = [(w2v_small_gender_bias.project_on_direction(word), word)\n",
    "                                    for word in all_gender_neutral_words]\n",
    "\n",
    "neutral_words_gender_projections.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_words_gender_projections[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_words_gender_projections[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 500 male-biased and top 500 female-biased words\n",
    "\n",
    "GenderBiasWE.plot_most_biased_clustering(w2v_small_gender_bias, w2v_small_gender_debias);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In the paper they got a stronger result, 92.5% accuracy for the debiased model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíé Strong words form the paper (emphasis mine):\n",
    "\n",
    "> The experiments ...\n",
    "reveal a **systematic bias** found in the embeddings,\n",
    "which is **independent of the gender direction**.\n",
    "\n",
    "\n",
    "> The implications are alarming: while suggested\n",
    "debiasing methods work well at removing the gender direction, the **debiasing is mostly superficial**.\n",
    "The bias stemming from world stereotypes and\n",
    "learned from the corpus is **ingrained much more\n",
    "deeply** in the embeddings space.\n",
    "\n",
    "\n",
    "> .. real concern from biased representations is **not the association** of a concept with\n",
    "words such as ‚Äúhe‚Äù, ‚Äúshe‚Äù, ‚Äúboy‚Äù, ‚Äúgirl‚Äù **nor** being\n",
    "able to perform **gender-stereotypical word analogies**... algorithmic discrimination is more likely to happen by associating one **implicitly gendered** term with\n",
    "other implicitly gendered terms, or picking up on\n",
    "**gender-specific regularities** in the corpus by learning to condition on gender-biased words, and generalizing to other gender-biased words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/banner.png)\n",
    "\n",
    "# üíéüíé Meta \"So What?\" - II\n",
    "\n",
    "## Can we debias at all a word embedding?\n",
    "\n",
    "## Under some downstream use-cases, maybe the bias in the word embedding is desirable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/banner.png)\n",
    "\n",
    "# üíª Your Turn!\n",
    "\n",
    "## Explore bias in word embedding by other groups (such as race and religious)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.** Use the Tolga's direct bias measure. Use the [`ethically.we.BiasWordEmbedding`](http://docs.ethically.ai/word-embedding-bias.html#ethically.we.bias.BiasWordEmbedding) class. We used `GenderBiasWE` which uses `BiasWordEmbedding` for the gender bias.\n",
    "\n",
    "For example, that's how we would use `BiasWordEmbedding` to analys the gender bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethically.we import BiasWordEmbedding\n",
    "\n",
    "gender_bias_we = BiasWordEmbedding(w2v_small, only_lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLUKBASI_DATA['gender']['definitional_pairs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíéüíéüíé identify the direction\n",
    "gender_bias_we._identify_direction(positive_end='she',\n",
    "                                   negative_end='he',\n",
    "                                   definitional=BOLUKBASI_DATA['gender']['definitional_pairs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLUKBASI_DATA['gender']['neutral_profession_names'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_bias_we.plot_projection_scores(BOLUKBASI_DATA['gender']['neutral_profession_names']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_bias_we.calc_direct_bias(BOLUKBASI_DATA['gender']['neutral_profession_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.** Open the [word embedding demo page in `ethically` documentation](http://docs.ethically.ai/notebooks/demo-words-embedding-bias.html#it-is-possible-also-to-expirements-with-new-target-word-sets-as-in-this-example-citizen-immigrant), and look on the use of the function [`calc_weat_pleasant_unpleasant_attribute`](). What was the attempt in that experiment? What was the result? Can you come up with other experiments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethically.we import calc_weat_pleasant_unpleasant_attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/banner.png)\n",
    "\n",
    "# Additional Related Work\n",
    "\n",
    "- Brunet, M. E., Alkalay-Houlihan, C., Anderson, A., & Zemel, R. (2018). [Understanding the Origins of Bias in Word Embeddings](https://arxiv.org/pdf/1810.03611.pdf). arXiv preprint arXiv:1810.03611.\n",
    "\n",
    "- Zhao, J., Wang, T., Yatskar, M., Cotterell, R., Ordonez, V., & Chang, K. W. (2019). [Gender Bias in Contextualized Word Embeddings](https://arxiv.org/pdf/1904.03310.pdf). arXiv preprint arXiv:1904.03310.\n",
    "\n",
    "\n",
    "- Complete example of using `ethically` with Word2Vec, GloVe and fastText: http://docs.ethically.ai/notebooks/demo-gender-bias-words-embedding.html\n",
    "\n",
    "\n",
    "# The Bigger Picture\n",
    "\n",
    "1. FAT community - Fairness, Accountability, and Transparency\n",
    "   - [ACM FAT*](https://fatconference.org)\n",
    "   - [FATML](http://www.fatml.org)\n",
    "   - [ML Fairness Book](https://fairmlbook.org)\n",
    "\n",
    "\n",
    "2. NLP - around dozen of papers on this field until the last few months, but nowdays plenty of work is done. See: [1st ACL Workshop on Gender Bias for Natural Language Processing](https://genderbiasnlp.talp.cat/), [NAACL 2019](https://naacl2019.org/)\n",
    "\n",
    "\n",
    "3. Tools: [ethically](https://docs.ethically.ai), [IBMM AIF360](https://aif360.mybluemix.net/)\n",
    "\n",
    "\n",
    "# üíéüíé Takeaways - Be Responsible\n",
    "\n",
    "1. Think about your **downstream app**\n",
    "\n",
    "2. Think about your **measurements** (aka \"what is a good system?\")\n",
    "\n",
    "3. Think about your **data** (corpus building, selection bias, train vs. validation vs. test datasets)\n",
    "\n",
    "4. Think about your impact on individuals, groups, society, and humanity\n",
    "\n",
    "# Guidlines and Checklists\n",
    "\n",
    "- [Google - People + AI Guidebook](https://pair.withgoogle.com/)\n",
    "\n",
    "- [Google - Responsible AI Practices](https://ai.google/responsibilities/responsible-ai-practices/)\n",
    "\n",
    "- [Denon - Ethics Checklist for Data Scientists](http://deon.drivendata.org/)\n",
    "\n",
    "\n",
    "![](images/banner.png)\n",
    "\n",
    "<center><h1>THE END!</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

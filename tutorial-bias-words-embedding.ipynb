{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Responsible Data Science\n",
    "# Bias in Words Embeddings\n",
    "\n",
    "### Powerd by [`ethically`](https://docs.ethically.ai/) - Toolkit for Auditing and Mitigating Bias and Fairness of Machine Learning Systems üîéü§ñüîß\n",
    "\n",
    "## by Shlomi Hod\n",
    "\n",
    "\n",
    "![](images/banner.png)\n",
    "\n",
    "### Legend:\n",
    "# üíé Important\n",
    "# üõ†Ô∏è Setup/Technical (aka the code is not important)\n",
    "# ü¶Ñ Out of scope \n",
    "\n",
    "![](images/banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install `ethically`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user ethically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation: Why to Learn Word Embeddings?\n",
    "\n",
    "### One-Hot Representation\n",
    "\n",
    "![](https://www.tensorflow.org/images/audio-image-text.png)\n",
    "<small>Source: [Tensorflow Documentation](https://www.tensorflow.org/tutorials/representation/word2vec)</small>\n",
    "\n",
    "\n",
    "## üíé Idea: Embedding a word in a n-dimensional space\n",
    "\n",
    "### Distributional Hypothesis\n",
    "> \"a word is characterized by the company it keeps\" - John Rupert Firth\n",
    "\n",
    "#### Training (ot of scope): using *word-context* relationships from a corpus\n",
    "\n",
    "### Distance ~ Meaning Similarity\n",
    "\n",
    "## ü¶Ñ Examples (algorithms and pre-trained models)\n",
    "- [Word2Vec](https://code.google.com/archive/p/word2vec/)\n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "- [fastText](https://fasttext.cc/)\n",
    "- [ELMo](https://allennlp.org/elmo) (contextualized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's play with Word2Vec words embedding...!\n",
    "\n",
    "[Word2Vec](https://code.google.com/archive/p/word2vec/) - Google News - 100B tokens, 3M vocab, cased, 300d vectors - only lowercase vocab extracted\n",
    "\n",
    "Loaded using [ethically](http://docs.ethically.ai) package, the function [`ethically.we.load_w2v_small`]() returns a [gensim](https://radimrehurek.com/gensim/)'s [KeyedVectors](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethically.we import load_w2v_small\n",
    "\n",
    "w2v_small = load_w2v_small()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary size\n",
    "\n",
    "len(w2v_small.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vector of the word \"home\"\n",
    "\n",
    "print('home =', w2v_small['home'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the words embedding dimension, in this case, is 300\n",
    "\n",
    "len(w2v_small['home'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the words are normalized (=have norm equal to one as vectors)\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "norm(w2v_small['home'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíé Demo - Mesuring Distance between Words\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7e/Sphere_wireframe_10deg_6r.svg/480px-Sphere_wireframe_10deg_6r.svg.png)\n",
    "\n",
    "ü¶Ñ Technical term: [cosine similariy](https://en.wikipedia.org/wiki/Cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small['cat'] @ w2v_small['cats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small['cat'] @ w2v_small['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small['cat'] @ w2v_small['cow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small['cat'] @ w2v_small['university']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Demo - Visualization Words Embedding in 2D using T-SNE \n",
    "\n",
    "<small>Source: [Google's Seedbank](https://research.google.com/seedbank/seed/pretrained_word_embeddings)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "# take the most common words in the corpus between 200 and 600\n",
    "words = [word for word in w2v_small.index2word[200:600]]\n",
    "\n",
    "# convert the words to vectors\n",
    "embeddings = [w2v_small[word] for word in words]\n",
    "\n",
    "# perform T-SNE\n",
    "words_embedded = TSNE(n_components=2).fit_transform(embeddings)\n",
    "\n",
    "# ... and visualize!\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, label in enumerate(words):\n",
    "    x, y = words_embedded[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',\n",
    "                 ha='right', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo - Most Similar\n",
    "\n",
    "What are the most simlar words (=closer) to a given word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small.most_similar('cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo - Doesn't Match\n",
    "\n",
    "Given a list of words, which one doesn't match?\n",
    "\n",
    "The word further away from the mean of all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small.doesnt_match('breakfast cereal dinner lunch'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo - Vector Arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nature + science = ?\n",
    "\n",
    "w2v_small.most_similar(positive=['nature', 'science'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíé More Vector Arithmetic\n",
    "\n",
    "![](https://www.tensorflow.org/images/linear-relationships.png)\n",
    "<small>Source: [Tensorflow Documentation](https://www.tensorflow.org/tutorials/representation/word2vec)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo - Vector Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# man:king :: woman:?\n",
    "# king - man + woman = ?\n",
    "\n",
    "w2v_small.most_similar(positive=['king', 'woman'],\n",
    "                       negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small.most_similar(positive=['east', 'west'],\n",
    "                       negative=['south'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo - Shift Context\n",
    "<small>Source: [Google's Seedbank](https://research.google.com/seedbank/seed/pretrained_word_embeddings)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_context(sentence, from_context, to_context):\n",
    "    new_sentence = []\n",
    "    for word in sentence.split():\n",
    "        if word in w2v_small:\n",
    "            word = w2v_small.most_similar(positive=[word, to_context],\n",
    "                                          negative=[from_context])[0][0]\n",
    "        new_sentence.append(word)\n",
    "\n",
    "    return ' '.join(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'restaurant serving coffee with cream and bread'\n",
    "\n",
    "print(shift_context(sentence, 'regular', 'fancy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/banner.png)\n",
    "\n",
    "# Gender Bias\n",
    "Keep in mind, the data is from Google News, the writers are professional journalists.\n",
    "\n",
    "### Bolukbasi Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam T. Kalai. [Man is to computer programmer as woman is to homemaker? debiasing word embeddings](https://arxiv.org/abs/1607.06520). NIPS 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender appropriate he-she analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# she:sister :: he:?\n",
    "# sister - she + he = ?\n",
    "\n",
    "w2v_small.most_similar(positive=['sister', 'he'],\n",
    "                       negative=['she'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "queen-king\n",
    "waitress-waiter\n",
    "sister-brother\n",
    "mother-father\n",
    "ovarian_cancer-prostate_cancer\n",
    "convent-monastery\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender stereotype he-she analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small.most_similar(positive=['nurse', 'he'],\n",
    "                       negative=['she'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sewing-carpentry\n",
    "nurse-surgeon\n",
    "blond-burly\n",
    "giggle-chuckle\n",
    "sassy-snappy\n",
    "volleyball-football\n",
    "register_nurse-physician\n",
    "interior_designer-architect\n",
    "feminism-conservatism\n",
    "vocalist-guitarist\n",
    "diva-superstar\n",
    "cupcakes-pizzas\n",
    "housewife-shopkeeper\n",
    "softball-baseball\n",
    "cosmetics-pharmaceuticals\n",
    "petite-lanky\n",
    "charming-affable\n",
    "hairdresser-barber\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíé Gender Direction\n",
    "\n",
    "# $\\overrightarrow{she} - \\overrightarrow{he}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_direction = w2v_small['she'] - w2v_small['he']\n",
    "\n",
    "gender_direction /= norm(gender_direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that all the vectors are normalized!\n",
    "\n",
    "from numpy.testing import assert_almost_equal\n",
    "\n",
    "length_vectors = norm(w2v_small.vectors, axis=1)\n",
    "\n",
    "assert_almost_equal(actual=length_vectors,\n",
    "                    desired=1,\n",
    "                    decimal=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_direction @ w2v_small['architect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_direction @ w2v_small['interior_designer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we calculate the gender direction using multiple definitional pair of words for better estimation (words may have more than one meaning):\n",
    "\n",
    "- woman - man\n",
    "- girl - boy\n",
    "- she - he\n",
    "- mother - father\n",
    "- daughter - son\n",
    "- gal - guy\n",
    "- female - male\n",
    "- her - his\n",
    "- herself - himself\n",
    "- Mary - John"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Gender Analogies\n",
    "\n",
    "### a:x::b:y when a-b = `gender_direction`\n",
    "### x - y ~ gender_direction\n",
    "\n",
    "#### How?\n",
    "1. Look for two words that are close to each other, with distance smaller than 1 (think why)\n",
    "2. Take their difference, and normalize\n",
    "3. Project the normalized difference on the gender direction, and order by the magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethically.we import GenderBiasWE\n",
    "\n",
    "w2v_small_gender_bias = GenderBiasWE(w2v_small,\n",
    "                                     only_lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython import display\n",
    "\n",
    "# the first line is for forcing the display of all the 150 rows\n",
    "with pd.option_context('display.max_rows', 150):\n",
    "    display.display(w2v_small_gender_bias.generate_analogies(150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíé So What?\n",
    "\n",
    "### Downstream Application\n",
    "\n",
    "### Toy Example - Search Engine Ranking\n",
    "\n",
    "- \"MIT PhD. Student\"\n",
    "- \"doctoral candidate\" ~ \"PhD. student\"\n",
    "- John:computer programmer :: Mary:homemaker\n",
    "\n",
    "### Universal Embeddings\n",
    "- Pre-trained on a large corpus\n",
    "- Plugged in downstream task models (sentimental analysis, classification, translation ‚Ä¶)\n",
    "- Improvement of performances\n",
    "\n",
    "### State of the Art\n",
    "[The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)\n",
    "](http://jalammar.github.io/illustrated-bert/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Bias in Words Embedding\n",
    "\n",
    "# Think-Pair-Shar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "# Basic Ideas: Use neutral-gender words!\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "# Professions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethically.we.data import BOLUKBASI_DATA\n",
    "\n",
    "neutral_profession_names = BOLUKBASI_DATA['gender']['neutral_profession_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_profession_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small[neutral_profession_names[0]] @ gender_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "f, ax = plt.subplots(1, figsize=(10, 10))\n",
    "\n",
    "w2v_small_gender_bias.plot_projection_scores(n_extreme=20, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Bias\n",
    "\n",
    "1. Project each **neutral profession names** on the gender direction\n",
    "2. Calculate the absolute value of each projection\n",
    "3. Average it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_profession_projections = [w2v_small[word] @ w2v_small_gender_bias.direction\n",
    "                                  for word in neutral_profession_names]\n",
    "\n",
    "abs_neutral_profession_projections = [abs(proj) for proj in neutral_profession_projections]\n",
    "\n",
    "sum(abs_neutral_profession_projections) / len(abs_neutral_profession_projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_bias.calc_direct_bias()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indirect Bias - EXTRA\n",
    "Similarity due to shared \"gender direction\" projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_bias.generate_closest_words_indirect_bias('softball',\n",
    "                                                           'football')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation of neutral profession projection between Word2Vec and FastText\n",
    "\n",
    "![](http://docs.ethically.ai/_images/demo-words-embedding-bias_50_0.png)\n",
    "\n",
    "(can be generated with the method `GenderBiasWE.plot_bias_across_words_embeddings`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debias\n",
    "\n",
    "### Neutralize\n",
    "\n",
    "In this case, we will remove the gender projection from all the words, except the neutral-gender ones, and then normalize.\n",
    "\n",
    "ü¶Ñ We need to \"learn\" what are the gender-specific words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias = w2v_small_gender_bias.debias(method='neutralize', inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('home:',\n",
    "      'before =', w2v_small_gender_bias.model['home'] @ w2v_small_gender_bias.direction,\n",
    "      'after = ', w2v_small_gender_debias.model['home'] @ w2v_small_gender_debias.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('man:',\n",
    "      'before =', w2v_small_gender_bias.model['man'] @ w2v_small_gender_bias.direction,\n",
    "      'after = ', w2v_small_gender_debias.model['man'] @ w2v_small_gender_debias.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('woman:',\n",
    "      'before =', w2v_small_gender_bias.model['woman'] @ w2v_small_gender_bias.direction,\n",
    "      'after = ', w2v_small_gender_debias.model['woman'] @ w2v_small_gender_debias.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.calc_direct_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(10, 10))\n",
    "\n",
    "w2v_small_gender_debias.plot_projection_scores(n_extreme=20, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equalize\n",
    "\n",
    "- Do you see that `man` and `woman` have a different projection on the gender direction? \n",
    "\n",
    "- It might cause to different similarity (distance) to neutral words, such as to `kitchen`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.model['man'] @ w2v_small_gender_debias.model['kitchen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.model['woman'] @ w2v_small_gender_debias.model['kitchen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLUKBASI_DATA['gender']['equalize_pairs'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Debias = Neutralize + Equalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias = w2v_small_gender_bias.debias(method='hard', inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('home:',\n",
    "      'before =', w2v_small_gender_bias.model['home'] @ w2v_small_gender_bias.direction,\n",
    "      'after = ', w2v_small_gender_debias.model['home'] @ w2v_small_gender_debias.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('man:',\n",
    "      'before =', w2v_small_gender_bias.model['man'] @ w2v_small_gender_bias.direction,\n",
    "      'after = ', w2v_small_gender_debias.model['man'] @ w2v_small_gender_debias.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('woman:',\n",
    "      'before =', w2v_small_gender_bias.model['woman'] @ w2v_small_gender_bias.direction,\n",
    "      'after = ', w2v_small_gender_debias.model['woman'] @ w2v_small_gender_debias.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.calc_direct_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.model['man'] @ w2v_small_gender_debias.model['kitchen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.model['woman'] @ w2v_small_gender_debias.model['kitchen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(10, 10))\n",
    "\n",
    "w2v_small_gender_debias.plot_projection_scores(n_extreme=20, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è the first line is for forcing the display of all the 150 rows\n",
    "with pd.option_context('display.max_rows', 100):\n",
    "    display.display(w2v_small_gender_debias.generate_analogies(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Preformances\n",
    "\n",
    "After debiasing, the performance of the words embedding, using standard benchmarks, get only slightly worse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_bias.evaluate_words_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.evaluate_words_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/banner.png)\n",
    "\n",
    "# üíé So What?\n",
    "\n",
    "We removed the gender bias, **as we defined it**, in a words embedding - Is there any impact on a downstream application?\n",
    "\n",
    "\n",
    "### Zhao, J., Wang, T., Yatskar, M., Ordonez, V., & Chang, K. W. (2018). [Gender bias in coreference resolution: Evaluation and debiasing methods](https://par.nsf.gov/servlets/purl/10084252). NAACL-HLT 2018.\n",
    "\n",
    "\n",
    "#### WinoBias Dataset\n",
    "![](images/coref-example.png)\n",
    "\n",
    "\n",
    "#### Stereotypical Occupations\n",
    "![](images/coref-occupations.png)\n",
    "\n",
    "#### Results\n",
    "![](images/coref-results.png)\n",
    "\n",
    "\n",
    "EE = UW End-to-end Neural Coreference Resolution System\n",
    "\n",
    "\n",
    "### Zhao, J., Zhou, Y., Li, Z., Wang, W., & Chang, K. W. (2018). [Learning gender-neutral word embeddings](https://arxiv.org/pdf/1809.01496.pdf). EMNLP 2018.\n",
    "\n",
    "#### Another debias method (tailor-made for GloVe training process)\n",
    "\n",
    "![](images/gn-glove-results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/banner.png)\n",
    "\n",
    "# üíé Have we really removed the bias?\n",
    "\n",
    "Let's look on another metric, called **WEAT** (Word Embedding Association Test) which is inspired by **IAT** (Implicit-Association Test) from Pyschology.\n",
    "\n",
    "### Caliskan, A., Bryson, J. J., & Narayanan, A. (2017). [Semantics derived automatically from language corpora contain human-like biases.](http://www.cs.bath.ac.uk/~jjb/ftp/CaliskanEtAl-authors-full.pdf) Science, 356(6334), 183-186.\n",
    "\n",
    "\n",
    "### Ingredients\n",
    "\n",
    "1. Target words (e.g., Male ve. Female)\n",
    "\n",
    "2. Attribute words (e.g., Math vs. Arts)\n",
    "\n",
    "### üõ†Ô∏è Recipe\n",
    "\n",
    "#### Part I\n",
    "For each word in of a target word (e.g., `he`)\n",
    "1. calc the mean similarity for every word in the **first** attribute words (e.g., `Math`)\n",
    "2. calc the mean similarity for every word in the **second** attribute words (e.g., `Arts`)\n",
    "3. calc the difference - a **measure of the association of one target words to the attributes** (e.g., `he` will be positive)\n",
    "\n",
    "**Association of one attribute words:** Mean of `he` @ `[science, technology, ...]` MINUS Mean of `he` @ `[poetry, dance, ...]`\n",
    "\n",
    "\n",
    "#### Part II\n",
    "For each taget words group, sum the **measure of the association**, and calc the difference - this is the **WEAT** score\n",
    "\n",
    "- Sum of **association of one attribute word** `[he, brother, ...]`\n",
    "- Minus\n",
    "- Sum of **association of one attribute word** `[she, sister, ...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethically.we.weat import WEAT_DATA\n",
    "\n",
    "weat_gender_science_arts = WEAT_DATA[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weat_gender_science_arts['first_attribute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weat_gender_science_arts['second_attribute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weat_gender_science_arts['first_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weat_gender_science_arts['second_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethically.we import calc_all_weat\n",
    "\n",
    "calc_all_weat(w2v_small_gender_bias.model, filter_by='model', with_original_finding=True,\n",
    "              with_pvalue=True, pvalue_kwargs={'method': 'approximate'}).iloc[7:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note: Our results are weaker because we use a reduced Word2Vec \n",
    "\n",
    "\n",
    "#### Results from the Paper (computed on the complete Word2Vec):\n",
    "\n",
    "![](images/weat-w2v.png)\n",
    "\n",
    "\n",
    "#### Caveat about comparing WEAT to the IAT\n",
    "\n",
    "- Individuals (IAT) vs. Words (WEAT)\n",
    "- Therefore, the meaning of the effect size and p-value is totally different!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's go back to our question - did we removed the bias?\n",
    "\n",
    "### Gonen, H., & Goldberg, Y. (2019). [Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them](https://arxiv.org/pdf/1903.03862.pdf). arXiv preprint arXiv:1903.03862.\n",
    "\n",
    "They used multiple methods, we'll show only two:\n",
    "1. WEAT\n",
    "2. Neutral words clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_bias.calc_direct_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.calc_direct_bias()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. WEAT - before and after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_all_weat(w2v_small_gender_bias.model, filter_by='model', with_original_finding=True,\n",
    "              with_pvalue=True, pvalue_kwargs={'method': 'approximate'}).iloc[7:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_all_weat(w2v_small_gender_debias.model, filter_by='model', with_original_finding=True,\n",
    "              with_pvalue=True, pvalue_kwargs={'method': 'approximate'}).iloc[7:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In the paper they got a stronger result, probably because they used the complete Word2Vec (in this example, p-value of 0.0467)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Clustering Neutral Gender Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vocab = set(w2v_small_gender_bias.model.vocab.keys())\n",
    "\n",
    "# ü¶Ñ how we got these words - read the Bolukbasi's paper for details\n",
    "all_gender_specific_words = set(BOLUKBASI_DATA['gender']['specific_full_with_definitional'])\n",
    "\n",
    "all_gender_neutral_words = w2v_vocab - all_gender_specific_words\n",
    "\n",
    "print('#vocab =', len(w2v_vocab),\n",
    "      '#specific =', len(all_gender_specific_words),\n",
    "      '#neutral =', len(all_gender_neutral_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_words_gender_projections = [(w2v_small_gender_bias.project_on_direction(word), word)\n",
    "                                    for word in all_gender_neutral_words]\n",
    "\n",
    "neutral_words_gender_projections.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_words_gender_projections[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_words_gender_projections[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, sorted_biased_neutral_words = zip(*neutral_words_gender_projections)\n",
    "\n",
    "female_biased_neutral_words = sorted_biased_neutral_words[-500:]\n",
    "male_biased_neutral_words = sorted_biased_neutral_words[:500]\n",
    "\n",
    "biased_neutral_words = female_biased_neutral_words + male_biased_neutral_words\n",
    "\n",
    "y_gender = [False] * 500 + [True] * 500\n",
    "\n",
    "len(biased_neutral_words), len(y_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Plotting Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def plot_clustered(model, biased_neutral_words, y_gender, ax=None):\n",
    "    \n",
    "    if ax is None:\n",
    "        f, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "\n",
    "    vectors = [model[word] for word in biased_neutral_words]\n",
    "    \n",
    "    y_cluster = KMeans(n_clusters=2, random_state=0).fit_predict(vectors)\n",
    "\n",
    "    embedded_vectors = TSNE(n_components=2, random_state=0).fit_transform(vectors)\n",
    "\n",
    "    ax.scatter(embedded_vectors[:, 0],\n",
    "               embedded_vectors[:, 1],\n",
    "               c=y_cluster)\n",
    "    \n",
    "    return accuracy_score(y_gender, y_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "\n",
    "acc_biased = plot_clustered(w2v_small_gender_bias.model, biased_neutral_words, y_gender, ax=axes[0])\n",
    "axes[0].set_title(f'Biased - Acc={acc_biased}')\n",
    "\n",
    "acc_debiased = plot_clustered(w2v_small_gender_debias.model, biased_neutral_words, y_gender, ax=axes[1])\n",
    "axes[1].set_title(f'Debiased - Acc={acc_debiased}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In the paper they got a stronger result, 92.5% accuracy for the debiased model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíé Strong words form the paper (my emphasis):\n",
    "\n",
    "> The experiments ...\n",
    "reveal a **systematic bias** found in the embeddings,\n",
    "which is **independent of the gender direction**.\n",
    "\n",
    "\n",
    "> The implications are alarming: while suggested\n",
    "debiasing methods work well at removing the gender direction, the **debiasing is mostly superficial**.\n",
    "The bias stemming from world stereotypes and\n",
    "learned from the corpus is **ingrained much more\n",
    "deeply** in the embeddings space.\n",
    "\n",
    "\n",
    "> .. real concern from biased representations is **not the association** of a concept with\n",
    "words such as ‚Äúhe‚Äù, ‚Äúshe‚Äù, ‚Äúboy‚Äù, ‚Äúgirl‚Äù **nor** being\n",
    "able to perform **gender-stereotypical word analogies**... algorithmic discrimination is more likely to happen by associating one **implicitly gendered** term with\n",
    "other implicitly gendered terms, or picking up on\n",
    "**gender-specific regularities** in the corpus by learning to condition on gender-biased words, and generalizing to other gender-biased words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/banner.png)\n",
    "\n",
    "# Your Turn!\n",
    "\n",
    "## Explore bias in words embedding by other groups (such as race and religious)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.** Use the Tolga's direct bias measure. Use the [`ethically.we.BiasWordsEmbedding`](http://docs.ethically.ai/words-embedding-bias.html#ethically.we.bias.BiasWordsEmbedding) class. We used `GenderBiasWE` which uses `BiasWordsEmbedding` for the gender bias.\n",
    "\n",
    "For example, that's how we would use `BiasWordsEmbedding` to analys the gender bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethically.we import BiasWordsEmbedding\n",
    "\n",
    "gender_bias_we = BiasWordsEmbedding(w2v_small, only_lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLUKBASI_DATA['gender']['definitional_pairs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíéüíéüíé identify the direction\n",
    "gender_bias_we._identify_direction(positive_end='she',\n",
    "                                   negative_end='he',\n",
    "                                   definitional=BOLUKBASI_DATA['gender']['definitional_pairs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLUKBASI_DATA['gender']['neutral_profession_names'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_bias_we.plot_projection_scores(BOLUKBASI_DATA['gender']['neutral_profession_names']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_bias_we.calc_direct_bias(BOLUKBASI_DATA['gender']['neutral_profession_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.** Open the [words embedding demo page in `ethically` documentation](http://docs.ethically.ai/notebooks/demo-words-embedding-bias.html#it-is-possible-also-to-expirements-with-new-target-word-sets-as-in-this-example-citizen-immigrant), and look on the use of the function [`calc_weat_pleasant_unpleasant_attribute`](). What was the attempt in that experiment? What was the result? Can you come up with other experiments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethically.we import calc_weat_pleasant_unpleasant_attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/banner.png)\n",
    "\n",
    "# More Related Work\n",
    "\n",
    "- Brunet, M. E., Alkalay-Houlihan, C., Anderson, A., & Zemel, R. (2018). [Understanding the Origins of Bias in Word Embeddings](https://arxiv.org/pdf/1810.03611.pdf). arXiv preprint arXiv:1810.03611.\n",
    "\n",
    "- Zhao, J., Wang, T., Yatskar, M., Cotterell, R., Ordonez, V., & Chang, K. W. (2019). [Gender Bias in Contextualized Word Embeddings](https://arxiv.org/pdf/1904.03310.pdf). arXiv preprint arXiv:1904.03310.\n",
    "\n",
    "\n",
    "- Complete example of using `ethically` with Word2Vec, GloVe and fastText: http://docs.ethically.ai/notebooks/demo-gender-bias-words-embedding.html\n",
    "\n",
    "\n",
    "# The Bigger Picture\n",
    "\n",
    "1. FAT community - Fairness, Accountability, and Transparency\n",
    "   - [ACM FAT*](https://fatconference.org)\n",
    "   - [FATML](http://www.fatml.org)\n",
    "   - [ML Fairness Book](https://fairmlbook.org)\n",
    "   \n",
    "2. NLP - around dozen of papers on this field (in the narrow sense)\n",
    "\n",
    "3. [`ethically` - https://docs.ethically.ai\n",
    "\n",
    "\n",
    "# üíé Takeaways - Be Responsible\n",
    "\n",
    "1. Think about your **downstream app**\n",
    "\n",
    "2. Think about your **measurements** (aka \"what is a good system?\")\n",
    "\n",
    "3. Think about your **data** (corpus building, selection bias, train vs. validation vs. test datasets)\n",
    "\n",
    "4. Think about your impact on individuals, groups, society, and humanity\n",
    "\n",
    "![](images/banner.png)\n",
    "\n",
    "<center><h1>THE END!</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
